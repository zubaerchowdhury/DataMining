{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "812a5a92",
   "metadata": {},
   "source": [
    "### Course Popularity - Final Project for CSC440: Data Mining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486403d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/Course_data/courses.sections.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m sections_file_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCourse_data\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourses.sections.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m sectionsTS_file_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCourse_data\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourses.sectionsTS.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m sections_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(sections_file_path)\n\u001b[1;32m     18\u001b[0m sections_ts_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(sectionsTS_file_path)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m========== Loaded Datasets ==========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/Course_data/courses.sections.csv'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ============================================================\n",
    "# 0. Load CSVs\n",
    "# ============================================================\n",
    "sections_file_path = Path(\"Data\") / \"Course_data\" / \"courses.sections.csv\"\n",
    "sectionsTS_file_path = Path(\"Data\") / \"Course_data\" / \"courses.sectionsTS.csv\"\n",
    "\n",
    "sections_df = pd.read_csv(sections_file_path)\n",
    "sections_ts_df = pd.read_csv(sectionsTS_file_path)\n",
    "\n",
    "print(\"\\n========== Loaded Datasets ==========\")\n",
    "print(\"courses.sections.csv info:\")\n",
    "print(sections_df.info(), \"\\n\")\n",
    "\n",
    "print(\"courses.sectionsTS.csv info:\")\n",
    "print(sections_ts_df.info(), \"\\n\")\n",
    "\n",
    "print(\"Static sections columns:\", sections_df.columns)\n",
    "print(\"Time-series columns:\", sections_ts_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40561bf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sections_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 131\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# 2. Run Reports for Both Datasets\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m sections_df \u001b[38;5;241m=\u001b[39m build_dq_report(sections_df\u001b[38;5;241m.\u001b[39mcopy(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourses.sections\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m sections_ts_df \u001b[38;5;241m=\u001b[39m build_dq_report(sections_ts_df\u001b[38;5;241m.\u001b[39mcopy(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourses.sectionsTS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sections_df' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Helper: Build Data Quality Report for a DataFrame\n",
    "# ============================================================\n",
    "\n",
    "def build_dq_report(df: pd.DataFrame, df_name: str):\n",
    "    \"\"\"\n",
    "    Perform data quality checks and print all results:\n",
    "    - Overview\n",
    "    - Null summary (all columns)\n",
    "    - Dtype summary\n",
    "    - Numeric summary (all numeric columns)\n",
    "    - Categorical summary (all categorical columns, full value counts)\n",
    "    - Type coercion results\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n============================================================\")\n",
    "    print(f\"############  DATA QUALITY REPORT: {df_name}  ############\")\n",
    "    print(\"============================================================\\n\")\n",
    "\n",
    "    # ---------- Overview ----------\n",
    "    print(\"---- Overview ----\")\n",
    "    overview = pd.DataFrame({\n",
    "        \"n_rows\": [df.shape[0]],\n",
    "        \"n_columns\": [df.shape[1]],\n",
    "        \"n_duplicates\": [df.duplicated().sum()]\n",
    "    })\n",
    "    print(overview, \"\\n\")\n",
    "\n",
    "    # ---------- Null Summary (All Columns) ----------\n",
    "    print(\"---- Null Summary (All Columns) ----\")\n",
    "    null_summary = pd.DataFrame({\n",
    "        \"column\": df.columns,\n",
    "        \"dtype\": df.dtypes.values.astype(str),\n",
    "        \"null_count\": df.isna().sum().values,\n",
    "        \"null_pct\": (df.isna().mean() * 100).values\n",
    "    }).sort_values(\"null_pct\", ascending=False)\n",
    "\n",
    "    print(null_summary.to_string(index=False), \"\\n\")\n",
    "\n",
    "    # ---------- Dtype Summary (All Types) ----------\n",
    "    print(\"---- Dtype Summary ----\")\n",
    "    dtype_summary = (\n",
    "        null_summary[[\"column\", \"dtype\"]]\n",
    "        .groupby(\"dtype\")\n",
    "        .agg(n_columns=(\"column\", \"count\"),\n",
    "             columns=(\"column\", lambda x: \", \".join(x)))\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(dtype_summary.to_string(index=False), \"\\n\")\n",
    "\n",
    "    # ---------- Numeric Summary (All Numeric Columns) ----------\n",
    "    print(\"---- Numeric Summary ----\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        numeric_summary = df[numeric_cols].describe().T\n",
    "        numeric_summary[\"missing_count\"] = df[numeric_cols].isna().sum()\n",
    "        numeric_summary[\"missing_pct\"] = df[numeric_cols].isna().mean() * 100\n",
    "        print(numeric_summary.to_string(), \"\\n\")\n",
    "    else:\n",
    "        print(\"No numeric columns.\\n\")\n",
    "\n",
    "    # ---------- Categorical Summary (All Categorical Columns) ----------\n",
    "    print(\"---- Categorical Summary ----\")\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    cat_summary_rows = []\n",
    "    for col in cat_cols:\n",
    "        series = df[col]\n",
    "        top_values = \", \".join(\n",
    "            [f\"{idx}({val})\" for idx, val in series.value_counts(dropna=True).items()]\n",
    "        )\n",
    "\n",
    "        cat_summary_rows.append({\n",
    "            \"column\": col,\n",
    "            \"dtype\": str(series.dtype),\n",
    "            \"n_unique\": series.nunique(dropna=True),\n",
    "            \"missing_pct\": series.isna().mean() * 100,\n",
    "            \"value_counts\": top_values\n",
    "        })\n",
    "    if cat_summary_rows:\n",
    "        cat_summary = pd.DataFrame(cat_summary_rows)\n",
    "        print(cat_summary.to_string(index=False), \"\\n\")\n",
    "    else:\n",
    "        print(\"No categorical columns.\\n\")\n",
    "\n",
    "    # ---------- Type Coercion Checks ----------\n",
    "    print(\"---- Type Coercion Checks ----\")\n",
    "\n",
    "    expected_numeric_cols = [\n",
    "        \"capacity\", \"waitlistCapacity\", \"reservedSeatsAvailable\",\n",
    "        \"reservedSeatsCapacity\", \"year\", \"catalogNumber\", \"classNumber\"\n",
    "    ]\n",
    "    expected_datetime_cols = [\n",
    "        \"dateTimeRetrieved\", \"timeStart\", \"timeEnd\",\n",
    "        \"startDate\", \"endDate\"\n",
    "    ]\n",
    "\n",
    "    # Numeric Type Coercion\n",
    "    print(\"\\nNumeric Columns Coercion Results:\")\n",
    "    for col in expected_numeric_cols:\n",
    "        if col in df.columns:\n",
    "            before = df[col].notna().sum()\n",
    "            coerced = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            after = coerced.notna().sum()\n",
    "            print(\n",
    "                f\"{col}: non-null before={before}, after={after}, \"\n",
    "                f\"invalid converted={before - after}\"\n",
    "            )\n",
    "            df[col] = coerced\n",
    "\n",
    "    # Datetime Type Coercion\n",
    "    print(\"\\nDatetime Columns Coercion Results:\")\n",
    "    for col in expected_datetime_cols:\n",
    "        if col in df.columns:\n",
    "            before = df[col].notna().sum()\n",
    "            coerced = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "            after = coerced.notna().sum()\n",
    "            print(\n",
    "                f\"{col}: non-null before={before}, after={after}, \"\n",
    "                f\"invalid converted={before - after}\"\n",
    "            )\n",
    "            df[col] = coerced\n",
    "\n",
    "    print(\"\\n========== END OF REPORT FOR:\", df_name, \"==========\\n\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Run Reports for Both Datasets\n",
    "# ============================================================\n",
    "\n",
    "sections_df = build_dq_report(sections_df.copy(), \"courses.sections\")\n",
    "sections_ts_df = build_dq_report(sections_ts_df.copy(), \"courses.sectionsTS\")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nData quality analysis completed.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2ada7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sections_ts_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Fix duplicate / typo columns: waitlistAvailable vs wailistAvailable\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaitlistAvailable\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sections_ts_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwailistAvailable\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sections_ts_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      5\u001b[0m     col_good \u001b[38;5;241m=\u001b[39m sections_ts_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaitlistAvailable\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m     col_typo \u001b[38;5;241m=\u001b[39m sections_ts_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwailistAvailable\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sections_ts_df' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Fix duplicate / typo columns: waitlistAvailable vs wailistAvailable\n",
    "# ============================================================\n",
    "if \"waitlistAvailable\" in sections_ts_df.columns and \"wailistAvailable\" in sections_ts_df.columns:\n",
    "    col_good = sections_ts_df[\"waitlistAvailable\"]\n",
    "    col_typo = sections_ts_df[\"wailistAvailable\"]\n",
    "\n",
    "    # Where both have values and differ, count mismatches\n",
    "    both_non_null = col_good.notna() & col_typo.notna()\n",
    "    mismatches = (both_non_null & (col_good != col_typo)).sum()\n",
    "\n",
    "    print(f\"\\n[Data Fix] waitlistAvailable vs wailistAvailable:\")\n",
    "    print(f\"  Rows where BOTH are non-null: {both_non_null.sum()}\")\n",
    "    print(f\"  Rows where values MISMATCH:  {mismatches}\")\n",
    "\n",
    "    # Merge logic (intersection-style):\n",
    "    # - If both non-null and equal → keep that value\n",
    "    # - If only one is non-null       → use the non-null value\n",
    "    # - If both non-null and different → set NaN (and rely on later checks)\n",
    "    merged = np.where(\n",
    "        both_non_null & (col_good == col_typo),\n",
    "        col_good,                     # agree → keep\n",
    "        np.where(\n",
    "            col_good.notna() & ~col_typo.notna(),\n",
    "            col_good,                 # only good has value\n",
    "            np.where(\n",
    "                col_typo.notna() & ~col_good.notna(),\n",
    "                col_typo,             # only typo has value\n",
    "                np.nan                # mismatch or both null\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    sections_ts_df[\"waitlistAvailable\"] = merged\n",
    "\n",
    "    # Drop the bad column\n",
    "    sections_ts_df = sections_ts_df.drop(columns=[\"wailistAvailable\"])\n",
    "\n",
    "    print(\"  -> Merged into 'waitlistAvailable' and dropped 'wailistAvailable'.\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n[Data Fix] One or both columns 'waitlistAvailable' / 'wailistAvailable' not found; no merge performed.\\n\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2. Merge Static Course Metadata with Time-Series Observations\n",
    "# ===============================================================\n",
    "\n",
    "# Merge on courseInfo.classNumber (primary key)\n",
    "df = sections_ts_df.merge(\n",
    "    sections_df,\n",
    "    on=\"courseInfo.classNumber\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_static\")\n",
    ")\n",
    "\n",
    "# Convert timestamps\n",
    "df[\"dateTimeRetrieved\"] = pd.to_datetime(df[\"dateTimeRetrieved\"], errors=\"coerce\")\n",
    "df = df.sort_values([\"courseInfo.classNumber\", \"dateTimeRetrieved\"])\n",
    "\n",
    "# ===============================================================\n",
    "# 3. Feature Engineering\n",
    "# ===============================================================\n",
    "\n",
    "# ---- Temporal Features ----\n",
    "df[\"days_since_start\"] = df.groupby(\"courseInfo.classNumber\")[\"dateTimeRetrieved\"] \\\n",
    "                           .transform(lambda x: (x - x.min()).dt.total_seconds() / 86400)\n",
    "\n",
    "df[\"hour\"] = df[\"dateTimeRetrieved\"].dt.hour\n",
    "df[\"day_of_week\"] = df[\"dateTimeRetrieved\"].dt.dayofweek # 0=Monday, ..., 6=Sunday\n",
    "df[\"is_weekend\"] = df[\"day_of_week\"].isin([5,6]).astype(int) # 5=Saturday, 6=Sunday\n",
    "\n",
    "# Cyclical time encoding\n",
    "df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "df[\"dow_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "df[\"dow_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "\n",
    "# ---- Enrollment Features ----\n",
    "df[\"seats_taken\"] = df[\"capacity\"] - df[\"seatsAvailable\"]\n",
    "df[\"fill_pct\"] = df[\"seats_taken\"] / df[\"capacity\"].replace(0, np.nan)\n",
    "\n",
    "df[\"waitlist_ratio\"] = df[\"waitlistAvailable\"] / df[\"waitlistCapacity\"].replace(0, np.nan)\n",
    "\n",
    "# ---- Velocity Metrics ----\n",
    "df[\"fill_velocity_per_day\"] = df.groupby(\"courseInfo.classNumber\")[\"fill_pct\"].diff() / \\\n",
    "                              df.groupby(\"courseInfo.classNumber\")[\"days_since_start\"].diff()\n",
    "\n",
    "df[\"rolling_velocity_6h\"] = df.groupby(\"courseInfo.classNumber\")[\"fill_pct\"].transform(\n",
    "    lambda x: x.diff().rolling(6).mean()\n",
    ")\n",
    "\n",
    "# ---- Course-Level Features ----\n",
    "df[\"course_level\"] = df[\"catalogNumber\"] // 100\n",
    "df[\"is_online\"] = (df[\"classroom\"] == \"Online\").astype(int)\n",
    "\n",
    "df[\"timeStart\"] = pd.to_datetime(df[\"timeStart\"], errors=\"coerce\")\n",
    "df[\"early_morning\"] = (df[\"timeStart\"].dt.hour < 10).astype(int)\n",
    "df[\"evening\"] = (df[\"timeStart\"].dt.hour >= 17).astype(int)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 4. Modeling Dataset Construction\n",
    "# ===============================================================\n",
    "target = \"fill_pct\"\n",
    "\n",
    "feature_cols = [\n",
    "    \"days_since_start\", \"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\",\n",
    "    \"is_weekend\", \"seats_taken\", \"capacity\", \"waitlist_ratio\",\n",
    "    \"fill_velocity_per_day\", \"rolling_velocity_6h\",\n",
    "    \"course_level\", \"is_online\", \"early_morning\", \"evening\"\n",
    "]\n",
    "\n",
    "# One-hot encode subjectCode & academicCareer\n",
    "df = pd.get_dummies(df, columns=[\"subjectCode\", \"academicCareer\"], drop_first=True)\n",
    "\n",
    "feature_cols.extend([col for col in df.columns if col.startswith(\"subjectCode_\")])\n",
    "feature_cols.extend([col for col in df.columns if col.startswith(\"academicCareer_\")])\n",
    "\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target]\n",
    "\n",
    "# ===============================================================\n",
    "# 5. Time-Series Cross Validation (No Leakage)\n",
    "# ===============================================================\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mae_scores, rmse_scores, r2_scores = [], [], []\n",
    "\n",
    "model = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    mae_scores.append(mean_absolute_error(y_test, preds))\n",
    "    rmse_scores.append(mean_squared_error(y_test, preds, squared=False))\n",
    "    r2_scores.append(r2_score(y_test, preds))\n",
    "\n",
    "\n",
    "print(\"\\n========== Time-Series CV Results ==========\")\n",
    "print(\"MAE:\", np.mean(mae_scores))\n",
    "print(\"RMSE:\", np.mean(rmse_scores))\n",
    "print(\"R²:\", np.mean(r2_scores))\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 6. Business-Relevant Metric: ±5% Tolerance Accuracy\n",
    "# ===============================================================\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X)\n",
    "\n",
    "tolerance = 0.05\n",
    "within_tol = np.mean(np.abs(preds - y) <= tolerance)\n",
    "\n",
    "print(\"\\nWithin ±5% tolerance accuracy:\", within_tol)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 7. Popularity Metrics for Ranking Courses\n",
    "# ===============================================================\n",
    "course_groups = df.groupby(\"courseInfo.classNumber\")\n",
    "\n",
    "popularity = pd.DataFrame({\n",
    "    \"courseInfo.classNumber\": course_groups[\"courseInfo.classNumber\"].first(),\n",
    "    \"subjectCode\": course_groups[\"subjectCode\"].first(),\n",
    "    \"catalogNumber\": course_groups[\"catalogNumber\"].first(),\n",
    "    \"course_level\": course_groups[\"course_level\"].first(),\n",
    "    \n",
    "    # Popularity Scores:\n",
    "    \"final_fill_rate\": course_groups[\"fill_pct\"].last(),\n",
    "    \"fill_velocity_score\": course_groups[\"fill_velocity_per_day\"].mean(),\n",
    "    \"early_rush_score\": course_groups[\"fill_pct\"].nth(3),  # first few hours\n",
    "    \"waitlist_demand_score\": course_groups[\"waitlist_ratio\"].max()\n",
    "})\n",
    "\n",
    "# Composite Score\n",
    "popularity[\"popularity_score\"] = (\n",
    "    0.40 * popularity[\"final_fill_rate\"] +\n",
    "    0.30 * popularity[\"fill_velocity_score\"] +\n",
    "    0.20 * popularity[\"waitlist_demand_score\"] +\n",
    "    0.10 * popularity[\"early_rush_score\"]\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 8. Ranking Courses (CSC, ECE by 200/300/400 level)\n",
    "# ===============================================================\n",
    "target_subjects = [\"CSC\", \"ECE\"]\n",
    "filtered = popularity[popularity[\"subjectCode\"].isin(target_subjects)]\n",
    "\n",
    "ranked_output = {}\n",
    "\n",
    "for subj in target_subjects:\n",
    "    subj_df = filtered[filtered[\"subjectCode\"] == subj]\n",
    "    ranked_output[subj] = {}\n",
    "\n",
    "    for level in [200, 300, 400]:\n",
    "        lvl = subj_df[subj_df[\"course_level\"] == level]\n",
    "        ranked_output[subj][level] = lvl.sort_values(\n",
    "            \"popularity_score\", ascending=False\n",
    "        ).head(10)  # top 10\n",
    "\n",
    "# Print Sample\n",
    "print(\"\\n=========== Top CSC 200-Level Courses ===========\")\n",
    "print(ranked_output[\"CSC\"][200].head())\n",
    "\n",
    "print(\"\\n=========== Top ECE 300-Level Courses ===========\")\n",
    "print(ranked_output[\"ECE\"][300].head())\n",
    "\n",
    "# Optionally export results\n",
    "popularity.to_csv(\"course_popularity_scores.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
